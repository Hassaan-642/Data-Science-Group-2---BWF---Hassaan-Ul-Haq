{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb244dd-bc83-4e4e-bc01-905772ce5c78",
   "metadata": {},
   "source": [
    "#Regression Models using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0ed34-6292-4170-bb4a-9faf1434267c",
   "metadata": {},
   "source": [
    "##Explore different regression models and apply them using the sk learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b0d4e-830b-4a8d-b392-112e29c80f3f",
   "metadata": {},
   "source": [
    "###importing necessary libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbae77d4-493b-4ef3-9384-70eab546dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\Huawei\\Desktop\\Reg_BikeShareDay.csv')\n",
    "\n",
    "# Features and target\n",
    "X = data[['temp', 'atemp', 'hum', 'windspeed', 'season', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']]\n",
    "y = data['cnt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd33b3-cdbd-48c2-931e-4a813bbc94b6",
   "metadata": {},
   "source": [
    "###Split the data into train and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a83428-36ab-465c-a60d-9fed1ab48b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c6495-121b-4de7-8d55-c1308d08892b",
   "metadata": {},
   "source": [
    "### here test size = 0.2 means 80% data is used for training and remaning 20% is for testing. It is basically 80 20 split\n",
    "### random_state=42 means data split is reproducible every time the code is run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc850158-0e1f-4bf5-ac47-5d238d98b1e2",
   "metadata": {},
   "source": [
    "### Linear Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d84472f4-ddaf-4891-aa97-9e3968f97cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE(MeanSquaredError):  103991.59111747658\n",
      "Linear Regression MAE(MeanAbsoluteError): 240.70695019630313\n",
      "Linear Regression R-squared: -0.16655410884579291\n"
     ]
    }
   ],
   "source": [
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(\"Linear Regression MSE(MeanSquaredError): \", mse_lr)\n",
    "print(\"Linear Regression MAE(MeanAbsoluteError):\", mae_lr)\n",
    "print(\"Linear Regression R-squared:\", r2_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455abb0-6725-49bd-9813-114309ee0bf2",
   "metadata": {},
   "source": [
    "### Linear Regression models the relationship between a dependant and one or more independant variable using a linear equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f42f80-fe18-458f-bd1a-53480e5b3b1b",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c28cff0-ef60-4d7b-bfc8-fb8a5973ca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MSE(MeanSquaredError):  32814.52451729854\n",
      "Ridge Regression MAE(MeanAbsoluteError):  174.19557503548145\n",
      "Ridge Regression R-squared:  0.6318940984157864\n"
     ]
    }
   ],
   "source": [
    "model_ridge = Ridge(alpha=1.0)\n",
    "model_ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = model_ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(\"Ridge Regression MSE(MeanSquaredError): \", mse_ridge)\n",
    "print(\"Ridge Regression MAE(MeanAbsoluteError): \", mae_ridge)\n",
    "print(\"Ridge Regression R-squared: \", r2_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868fa92-310a-4273-9e0c-3728d3cf84dd",
   "metadata": {},
   "source": [
    "### Ridge regression is a technique that adds a penalty to the size of coefficents to prevent them from getting to large.\n",
    "### This helps the model reduces overfitting(i.e model learns train data too well including noise and detials, making it perform poorly on unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be3bca-875a-4c13-bee7-ecf5370bd42c",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "926c5d6d-291a-4b38-ba94-fcfcecee4771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MSE(MeanSquaredError):  79236.1081209185\n",
      "Lasso Regression MAE(MeanAbsoluteError):  201.79053781596588\n",
      "Lasso Regression R-squared:  0.11114729081327723\n"
     ]
    }
   ],
   "source": [
    "model_lasso = Lasso(alpha=0.1)\n",
    "model_lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = model_lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(\"Lasso Regression MSE(MeanSquaredError): \", mse_lasso)\n",
    "print(\"Lasso Regression MAE(MeanAbsoluteError): \", mae_lasso)\n",
    "print(\"Lasso Regression R-squared: \", r2_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced08f0-29c9-4955-ba07-3b669aeb4820",
   "metadata": {},
   "source": [
    "### Lasso regression adds a penalty to reduce some coefficents to zero, effectively selecting important features.\n",
    "### This helps simplify the model and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef61c0-bde7-4ba9-ac1b-3590ee78ea26",
   "metadata": {},
   "source": [
    "### SGD(Stochastic Gradient Descent) Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e01e6cc2-f9ab-40af-97fc-9b22a8f7ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Regressor MSE(MeanSquaredError):  34389.090805972286\n",
      "SGD Regressor MAE(MeanAbsoluteError):  177.01891931376846\n",
      "SGD Regressor R-squared:  0.6142309705228067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "model_sgd.fit(X_train, y_train)\n",
    "y_pred_sgd = model_sgd.predict(X_test)\n",
    "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
    "mae_sgd = mean_absolute_error(y_test, y_pred_sgd)\n",
    "r2_sgd = r2_score(y_test, y_pred_sgd)\n",
    "print(\"SGD Regressor MSE(MeanSquaredError): \",mse_sgd)\n",
    "print(\"SGD Regressor MAE(MeanAbsoluteError): \",mae_sgd)\n",
    "print(\"SGD Regressor R-squared: \",r2_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd7b28-8c16-4e9e-9fff-bbb0da8b236c",
   "metadata": {},
   "source": [
    "### SGD Regressor uses a method called Stochastic Gradient Descent to update the model step by step, making it efficent for large datasets.\n",
    "### It adjusts the model gradually, aiming to minimize errors in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24915164-8213-45b9-a4aa-cc22598e4eec",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca724c37-128a-4d6f-b910-2f4e99dd4d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR MSE(MeanSquaredError):  122014.66009289038\n",
      "SVR MAE(MeanAbsoluteError):  324.72961953144505\n",
      "SVR R-squared:  -0.36873281330978025\n"
     ]
    }
   ],
   "source": [
    "model_svr = SVR(kernel='rbf')\n",
    "model_svr.fit(X_train, y_train)\n",
    "y_pred_svr = model_svr.predict(X_test)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "print(\"SVR MSE(MeanSquaredError): \",mse_svr)\n",
    "print(\"SVR MAE(MeanAbsoluteError): \",mae_svr)\n",
    "print(\"SVR R-squared: \",r2_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abddc2-92cf-455c-a289-e1ca90750a02",
   "metadata": {},
   "source": [
    "### Support vector regression uses a flexible kernel to find the best line that fits within a margin of error.\n",
    "### it is effective for complex data and can handle outliers(a data point taht is much different from the other data points) well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75258b9-de39-477a-9905-359afb16c6a6",
   "metadata": {},
   "source": [
    "### Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f58e2fe7-bf1d-44dd-aa87-b7d1b4748fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Ridge Regression MSE(MeanSquaredError):  100709.64141231301\n",
      "Bayesian Ridge Regression MAE(MeanAbsoluteError):  294.35803870620003\n",
      "Bayesian Ridge Regression R_squared:  -0.12973794061100974\n"
     ]
    }
   ],
   "source": [
    "model_br = BayesianRidge()\n",
    "model_br.fit(X_train, y_train)\n",
    "y_pred_br = model_br.predict(X_test)\n",
    "mse_br = mean_squared_error(y_test, y_pred_br)\n",
    "mae_br = mean_absolute_error(y_test, y_pred_br)\n",
    "r2_br = r2_score(y_test, y_pred_br)\n",
    "print(\"Bayesian Ridge Regression MSE(MeanSquaredError): \", mse_br)\n",
    "print(\"Bayesian Ridge Regression MAE(MeanAbsoluteError): \", mae_br)\n",
    "print(\"Bayesian Ridge Regression R_squared: \", r2_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433e053-145a-49ca-97f4-61ea532847b7",
   "metadata": {},
   "source": [
    "### Bayesian Ridge Regression estimates the best-fit line by considering uncertainties in the coefficients and adding regularization(adding penalty to model's complexity to prevent it from overfitting). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c2646-d9d8-4afb-8554-c560a6bd2c7b",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "331a7eaf-1359-4625-9395-84cf84621c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regression MSE(MeanSquaredError):  62805.426318633894\n",
      "Elastic Net Regression MAE(MeanAbsoluteError):  245.87205297798158\n",
      "Elastic Net Regression R_squared:  0.29546295673995915\n"
     ]
    }
   ],
   "source": [
    "model_en = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "model_en.fit(X_train, y_train)\n",
    "y_pred_en = model_en.predict(X_test)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "mae_en = mean_absolute_error(y_test, y_pred_en)\n",
    "r2_en = r2_score(y_test, y_pred_en)\n",
    "print(\"Elastic Net Regression MSE(MeanSquaredError): \", mse_en)\n",
    "print(\"Elastic Net Regression MAE(MeanAbsoluteError): \", mae_en)\n",
    "print(\"Elastic Net Regression R_squared: \", r2_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961a73c-c9cf-4420-af94-20c838d62b7a",
   "metadata": {},
   "source": [
    "### Elastic net regression combines ridge and lasso penalties to balance between shrinking co_efficents and selecting important features.\n",
    "### It helps manage correlated features and simplifies the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
